# YOLO
2015년도에 나온 논문으로 Faster-R-CNN에 비해 6배 가량 빠른 속도를 보인 모델이다.
- 기존의 Object Detection과 가장 크게 구분되는 점\
  : region proposal 과 classification로 두단계 나누어 진행하던 방식에서 region proposal단계를 제거하고 한번에 수행하는 구조를 가진다.
- 장점
1. 간단한 처리과정으로 속도가 빠르다
2. Image 전체를 한번에 바라보는 방식으로 class에 대한 이해도가 높다. (낮은 False Positive)
3. Object에 대한 좀더 일반화된 특징을 학습한다.
- 단점
1. 상대적으로 낮은 정확도 ( 특히, 작은 object에 대해)
  
## Unitied Detection
> - 아래 그림은 yolo의 1 step구조이다.
> ![image](https://user-images.githubusercontent.com/70633080/103399159-03b62600-4b83-11eb-9e08-37e8092110a4.png)
> 1. 입력이미지를 s * s grid영역으로 나눈다. (실제 입력 이미지를 나누는 것이 아니다. 자세한 내용은 아래에서)
> 2. 각 grid영역에서 물체가 있을만한 영역에 해당하는 **B개의 Bounding box**를 예측한다.
>     - 이는 (x,y,w,h)로 나타내는데 (x,y)는 box의 중심점 좌표이며 , (w,h)는 넓이와 높이이다.
> 3. 해당 박스의 **신뢰도를 나타내는 Confidence**를 계산한다.
>     - 이는 해당 grid에 물체가 있을 확률 Pr(object)와 예측한박스와 ground truth box와 겹치는 영역을 비율로 나타내는 IOU를 곱해서 계산한다.
>     ![image](https://user-images.githubusercontent.com/70633080/103399292-b6868400-4b83-11eb-9739-25b079450ddf.png)
> 4. 다음으로 각 grid마다 C개의 class에 대해 해당 class일 확률을 계산한다. 수식은 아래와 같다.
>   - 이때, 기존의 object detection에서는 항상 class 수 + 1(배경)으로 분류하는데 yolo는 그렇지않다.\
>    ![image](https://user-images.githubusercontent.com/70633080/103399316-dcac2400-4b83-11eb-9571-72fffa50a4ac.png)
> - **이렇듯 yolo는 입력이미지를 grid로 나누고 각 grid 별 bbox와 classification을 동시에 수행한다.**

## Network Design
>![image](https://user-images.githubusercontent.com/70633080/103399360-141ad080-4b84-11eb-8ff4-e2891bedacf3.png)
> - 저자는 GoogleNet의 아키텍처에서 영감을 받았다.
>     - 24 Convolutional layers & 2 Fully Connected layers - 참고로 Fast YOLO는 위 디자인의 24의 convolutional layer를 9개의 convolutional layer로 대체
> - Inception block대신 단순한 conv로 구성했다.
> 1. 224 * 224 size의 image classification으로 pretrain시킨다.
> 2. 이후 입력이미지로 448 * 448 size의 image를 입력으로 받는다.
> 3. 앞쪽 20개의 conv layer는 고정하고 뒤의 4개의 layer만 object detection task에 맞게 학습한다.\
> ![image](https://user-images.githubusercontent.com/70633080/103399423-74aa0d80-4b84-11eb-92a5-4ca376ff0410.png)

## Inference Process
> ![image](https://user-images.githubusercontent.com/70633080/103399483-aae78d00-4b84-11eb-8e24-f75765ed33e8.png)
> - output은 7 * 7 * 30의 feature map이며 grid별 bbox와 confidence score, class별 예측값이 담겨있다.
> - 7 * 7 은 49개의 grid이며 각 index는 총 30차원의 vector값을 가진다.
> - 각각의 Grid cell은 B개의 bounding box를 가지고 있다. ( 논문에서 B=2)
> - 따라서 output vector중 앞의 5개는 해당 grid cell의 첫번째 bbox에 대한 값이다.
>     - x,y,w,h,c(confidence score)
> - 그 뒤 6~10번째 값은 두번째 bbox에 대한 값이다.
> - 나머지 20개의 값은 20개의 class에 대한 conditional class probability이다.\
> ![image](https://user-images.githubusercontent.com/70633080/103400690-64e0f800-4b89-11eb-8211-6c934eeb161f.png)
> - 첫번째 bbox의 confidence score와 각 conditional class probability를 곱하면 첫번째 bbox의 class specific confidence score가 나온다.
> - 두번째 bbox도 마찬가지이다.
> - 이러한 계산을 각 bbox에 대해 하게되면 총 98개 (49 * 2)의 class specific confidence score를 얻을 수 있다.
> - 이 98개의 cscs에 대해 각 20개의 class를 기준으로 Non-maximum supression을 하여 object에 대한 class 및 bbox location을 결정한다.

## Loss Function
> YOLO의 loss function을 이해한다.\
> Loss function을 보기 전, 몇가지 전제조건에 대해 이해한다.
> - 전제조건
> 1. Grid cell의 여러 bbox들 중, ground truth box와의 IOU가 가장 높은 bbox를 predictor로 설정한다.
> 2. 1의 기준에 따라 아래 기호들이 사용된다.
> ![image](https://user-images.githubusercontent.com/70633080/103400914-2b5cbc80-4b8a-11eb-8cfc-d817291e1789.png)
>     1. object가 존재하는 grid cell i의 predictor bounding box j
>     2. object가 존재하지 않는 grid cell i의 bounding box j
>     3. object가 존재하는 grid cell i\
> **ground truth box의 중심점이 어떤 grid cell 내부에 위치하게되면 그 grid cell에는 object가 존재한다고 여긴다.**
> ### Loss Function
> ![image](https://user-images.githubusercontent.com/70633080/103400974-6bbc3a80-4b8a-11eb-9ec7-835fa84c07ed.png)\
> (1) object가 존재하는 grid cell i 의 predictor bbox j에 대해 **x와 y의 loss**를 계산한다.\
> (2) object가 존재하는 grid cell i 의 predictor bbox j 에 대해 **w와 h의 loss**를 계산한다.\
>     (2)-1 큰  box에 대해서는 small deviation을 반영하기 위해 제곱근을 취한후, sum-squared error를 해준다. (같은 error라도 lager box의 경우 상대적으로 IOU에 영향을 적게 주도록 하는것)\
> (3) object가 **존재하는** grid cell i 의 predictor bbox j 에 대해 **confidence score의 loss**를 계산한다. ( C_i =1 )\
> (4) object가 **존재하지 않는** grid cell i 의 bbox j 에 대해 **confidence score의 loss**를 계산 (C_i=0)\
> (5) object가 존재하는 grid cell i 에 대해, **conditional class probability의 loss**를 계산한다. (Correct class c : p_i(c)=1, otherwise : p_i(c)=0)
> - lambda_coord : coordinates(x,y,w,h)에 대한 loss와 다른 loss들과의 균형을 위한 balancing parameter.
> - lambda_noobj : 물체가 있는 box와 없는 box간 균형을 위한 balancing parameter.

## Training
> 1. ImageNet 1000-class data set 으로 20개의 conv layer를 pre-training
> 2. Pre-training 이후 4 conv layers와 2 fc layer추가
> 3. BBOX의 w와 h는 image의 w와 h로 normalize ( 0 ~ 1사이값)
> 4. BBOX의 x와 y는 특정 grid cell 위치의 offset값을 사용한다. (0 ~ 1사이값)
> - learning parameter
>     - lambda_coord : 5, lambda_noobj : 0.5
>     - batch size : 64
>     - momentum : 0.9 , decay : 0.0005
>     - learning rate : 0.001 ~ 0.01로 epoch마다 천천히 상승.
>     - Dropout rate : 0.5
>     - Data augmentation : random scailing & translations of up to 20% of the original image size
>     - Activation function : leaky rectified linear activation\
> ![image](https://user-images.githubusercontent.com/70633080/103401548-9c04d880-4b8c-11eb-87b5-25be6f62ff28.png)
