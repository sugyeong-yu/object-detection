# YOLO
2015년도에 나온 논문으로 Faster-R-CNN에 비해 6배 가량 빠른 속도를 보인 모델이다.
- 기존의 Object Detection과 가장 크게 구분되는 점\
  : region proposal 과 classification로 두단계 나누어 진행하던 방식에서 region proposal단계를 제거하고 한번에 수행하는 구조를 가진다.
- 장점
1. 간단한 처리과정으로 속도가 빠르다
2. Image 전체를 한번에 바라보는 방식으로 class에 대한 이해도가 높다. (낮은 False Positive)
3. Object에 대한 좀더 일반화된 특징을 학습한다.
- 단점
1. 상대적으로 낮은 정확도 ( 특히, 작은 object에 대해)
  
## Unitied Detection
> - 아래 그림은 yolo의 1 step구조이다.
> ![image](https://user-images.githubusercontent.com/70633080/103399159-03b62600-4b83-11eb-9e08-37e8092110a4.png)
> 1. 입력이미지를 s * s grid영역으로 나눈다. (실제 입력 이미지를 나누는 것이 아니다. 자세한 내용은 아래에서)
> 2. 각 grid영역에서 물체가 있을만한 영역에 해당하는 **B개의 Bounding box**를 예측한다.
>     - 이는 (x,y,w,h)로 나타내는데 (x,y)는 box의 중심점 좌표이며 , (w,h)는 넓이와 높이이다.
> 3. 해당 박스의 **신뢰도를 나타내는 Confidence**를 계산한다.
>     - 이는 해당 grid에 물체가 있을 확률 Pr(object)와 예측한박스와 ground truth box와 겹치는 영역을 비율로 나타내는 IOU를 곱해서 계산한다.
>     ![image](https://user-images.githubusercontent.com/70633080/103399292-b6868400-4b83-11eb-9739-25b079450ddf.png)
> 4. 다음으로 각 grid마다 C개의 class에 대해 해당 class일 확률을 계산한다. 수식은 아래와 같다.
>   - 이때, 기존의 object detection에서는 항상 class 수 + 1(배경)으로 분류하는데 yolo는 그렇지않다.\
>    ![image](https://user-images.githubusercontent.com/70633080/103399316-dcac2400-4b83-11eb-9571-72fffa50a4ac.png)
> - **이렇듯 yolo는 입력이미지를 grid로 나누고 각 grid 별 bbox와 classification을 동시에 수행한다.**

## Network Design
>![image](https://user-images.githubusercontent.com/70633080/103399360-141ad080-4b84-11eb-8ff4-e2891bedacf3.png)
> - 저자는 GoogleNet의 아키텍처에서 영감을 받았다.
>     - 24 Convolutional layers & 2 Fully Connected layers - 참고로 Fast YOLO는 위 디자인의 24의 convolutional layer를 9개의 convolutional layer로 대체
> - Inception block대신 단순한 conv로 구성했다.
> 1. 224 * 224 size의 image classification으로 pretrain시킨다.
> 2. 이후 입력이미지로 448 * 448 size의 image를 입력으로 받는다.
> 3. 앞쪽 20개의 conv layer는 고정하고 뒤의 4개의 layer만 object detection task에 맞게 학습한다.\
> ![image](https://user-images.githubusercontent.com/70633080/103399423-74aa0d80-4b84-11eb-92a5-4ca376ff0410.png)\
> ![image](https://user-images.githubusercontent.com/70633080/103399483-aae78d00-4b84-11eb-8e24-f75765ed33e8.png)
> - output은 7 * 7 * 30의 feature map이며 grid별 bbox와 confidence score, class별 예측값이 담겨있다.
> - 7 * 7 은 49개의 grid이며 각 index는 총 30차원의 vector값을 가진다.
> - 각각의 Grid cell은 B개의 bounding box를 가지고 있다. ( 논문에서 B=2)
> - 따라서 output vector중 앞의 5개는 해당 grid cell의 첫번째 bbox에 대한 값이다.
>     - x,y,w,h,c(confidence score)
> - 그 뒤 6~10번째 값은 두번째 bbox에 대한 값이다.
> - 나머지 20개의 값은 20개의 class에 대한 conditional class probability이다.\
> ![image](https://user-images.githubusercontent.com/70633080/103400690-64e0f800-4b89-11eb-8211-6c934eeb161f.png)
> - 첫번째 bbox의 confidence score와 각 conditional class probability를 곱하면 첫번째 bbox의 class specific confidence score가 나온다.
> - 두번째 bbox도 마찬가지이다.
> - 이러한 계산을 각 bbox에 대해 하게되면 총 98개 (49 * 2)의 class specific confidence score를 얻을 수 있다.
> - 이 98개의 cscs에 대해 각 20개의 class를 기준으로 Non-maximum supression을 하여 object에 대한 class 및 bbox location을 결정한다.

## Training
> 
