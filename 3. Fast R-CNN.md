# Fast R-CNN
CNN fine tuning, boundnig box regression, classification을 모두 하나의 네트워크에서 학습시키는 " **end-to-end** 기법을 제시한 모델 "
## 핵심 아이디어
>  - SPP NET 의 한계점
>   1. 여러 단계의 학습
>   2. Fully connected layer 밖에 학습을 시키지 못함.\
>  이러한 한계점을 극복하고자한 모델이 Fast R-CNN

## 알고리즘
> 1. 전체이미지를 pretrain 된 CNN에 통과 , feature map 추출
> 2. Selective search를 통해 얻은 각각의 ROI에 대해 ROI Pooling 수행 , 고정된 크기의 feature vector 추출
> 3. feature vector는 fc layer를 통과, **2개의 branch로 나뉘게 된다.**
> 4. (1) 하나의 branch는 softmax를 통과, 해당 ROI가 어떤 물체인지 **classification**. (더이상 svm은 사용하지 않는다.)\
> (2) bounding box regression을 통해 selective search로 찾은 **박스위치 조정**.

> - step별로 쪼개어 학습을 진행하지 않고 **end-to-end**로 엮였다는 것이 가장 큰 포인트
> - 학습속도, 인퍼런스속도, 정확도 모두 향상

## Roi Pooling
> ### extract cnn feature map
>     1. 입력이미지에서 feature map 추출.
>     2. 추출된 feature map 은 지정된 H*W size
>     3. max pooling 수행, 항상 H*W size의 feature map.
>     4. feature map을 펼처 feature vector 추출.
>![image](https://user-images.githubusercontent.com/70633080/103200144-f2291000-492f-11eb-9e12-ac2d3d39f2b9.png)
> - Roi pooling은 spatial pyramid poooling 의 pyramid level1의 경우와 동일하다.
> - **input image size 와 feature map size 가 다를 경우**\
>   : 비율을 구해 ROI를 조절한 후 ROI Pooling을 진행.

## Multi Task Loss
> - 앞에서 얻은 feature vector로 classification과 bounding box regression을 적용하여 각각의 loss를 얻는다.
> - 이를 back propagation하여 전체모델을 학습시킨다.
> ### Multi Task Loss
> : classification loss 와 bounding box regression을 적절히 엮어주는 것
> #### 수식
> ![image](https://user-images.githubusercontent.com/70633080/103200675-54364500-4931-11eb-8421-d448af6cc508.png)\
> ![image](https://user-images.githubusercontent.com/70633080/103200792-a5463900-4931-11eb-8e8d-00ee6d62e370.png)\
> -**입력 P** : sotf max를 통해 얻은 K+1( 물체 k + 배경 1)개의 확률 값\
> -**u** : 해당 ROI의 ground truth label 값.
> 
> #### BB Regression
> : k+1개의 class에 대한 x,y,h,w값을 조정하는 t_k를 return.\
> ex) 사람일 경우 box를 n만큼, 고양이일 경우 m 만큼 조절해라.\
> ![image](https://user-images.githubusercontent.com/70633080/103201092-5c42b480-4932-11eb-85c2-fd9e01ed6e4e.png)
> #### loss function
> - loss 앞부분\
> ![image](https://user-images.githubusercontent.com/70633080/103201111-6ebcee00-4932-11eb-8014-800dcddb6c7c.png)\
> p와 u를 가지고 classification loss를 구한다. (log loss사용)
> - loss 뒷부분\
> BB Regression을 통해 얻는 loaa이다.\
> ![image](https://user-images.githubusercontent.com/70633080/103201168-9613bb00-4932-11eb-996d-dfb846f15894.png)
> - 입력 : 정답라벨에 해당하는 BBR예측값, ground truth 조절값
> - t_u : ground truth label에 해당하는 값
> - v : ground truth bounding box의 조절 값
> - x,y,w,h 각각에 대해 예측값과 라벨 값의 차이를 계산 한 후 smoothL1이라는 함수를 통과시킨 합을 계산한다.
> #### smooth L1
> ![image](https://user-images.githubusercontent.com/70633080/103201756-053ddf00-4934-11eb-82c1-c91ddb7c73e9.png)
> - 예측값 - 라벨값 <1\
> : 0.5x*2 로 L2 Distance를 계산한다.
> - 예측값 - 라벨값 <=1\
> : L1 Distance를 계산한다.
> - 라벨값과 지나치게 차이가 많이나는 outlier 예측값들이 발생, 이들을 L2로 계산할 경우 gradient exploding문제가 발생한다. 따라서 다음 함수를 추가한 것이다.

## Backpropogation through ROI Pooling layer
> - loss function을 구했으니 network 학습이 필요하다.
> - 네트워크를 어디까지 학습 시킬 것인가?
>     - SPP NET : feature map을 추출하는 CNN은 제외하고 SPP이후의 FC만 fine-tune하였다.\
>       이는 이미지로부터 특징을 뽑는 **가장 중요한 CNN**이 학습될 수 없어 성능향상에 제약이 있다는 주장이 있었다.
>     - 과연 ROI Pooling layer 이전까지 역전파를 전달할 수 있는가!
> ### 수식
> ![image](https://user-images.githubusercontent.com/70633080/103202306-631ef680-4935-11eb-9ee9-76655a139592.png)
> - x_i : CNN을 통해 추출된 feature map에서 하나의 feature 값 (실수)
> - 전체 Loss에 대해 이 x_i에 대한 편미분 값을 구하면 그 값이 x_i에 대한 loss 값이 되며 역전파 알고리즘을 수행할 수 있게 된다.
> 
> - feature map에서 ROI를 찾고 ROI Pooling을 적용하기 위해 h*w size의 grid로 나눈다.
>     - grid 를 sub-window라고 부른다. 
>     - j : 몇번째 sub-window인지를 나타내는 인덱스.
> - y_rj : ROI Pooling을 통과해 최종적으로 얻어진 output값. (하나의 실수)\
> ![image](https://user-images.githubusercontent.com/70633080/103202510-fa844980-4935-11eb-8d8b-d62719265535.png)
> - x_i가 최종 prediction 값에 영향을 주기 위해선 x_i가 속하는 모든 ROI의 sub-window에서 해당 x_i가 최댓값이어야 한다.
> - i*(r,j) : ROI와 sub-window index j 가 주어졌을때, 최대 feature 값의 index. = ROI Pooling을 통과하는 index값. 

## 정리
> -RoI Pooling을 통과한 이후 값에 대한 Loss는 이미 전체 Loss에 대한 yrj의 편미분 값으로 이미 계산이 되어 있다.그러므로 이를 중첩시키기만 하면 xi에 대한 loss를 구할 수 있는 것이다.
