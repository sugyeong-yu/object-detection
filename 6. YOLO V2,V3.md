# YOLO V2
기존 YOLO모델을 보완하여 정확도를 높인 YOLO V2모델을 제시한다.\
그리고 V2모델을 기반으로 9000종류의 물체를 구분할 수 있는 YOLO9000모델을 살펴본다.\
## 발전
- Batch Normalization적용
- 높은 해상도 이미지로 백본 CNN네트워크 fine tune
- Anchor Box개념 적용하여 학습 안정화
- 높은 해상도의 feature map을 낮은 해상도 feature map에 합치기

1. Batch Normalization의 적용
    - 기존 모델에서 Dropout Layer를 제거하고 Batch Normalization을 추가한다. > mAP 2% 증가

2. High Resolution Classifier
    - 기존 YOLO는 224 * 224 크기의 해상도로 학습된 VGG모델을 가져온 후, 448 * 448 크기의 이미지에 대해 OBJECT DETECTION을 수행하도록 되어있어 해상도가 맞지 않았다.\
    - 이를 학습 전 Image Classification 모델을 큰 해상도이미지에 대해 fine tuning함으로써 해결하였다. > mAP 4%
   
3. Convolutional With Anchor Boxes
    - 기존 YOLO에서 FC layer를 떼고 Fully Convolutional Network형태로 prediction을 계산한다. 또, Anchor Box의 개념을 도입한다.
    - 아래 그림을 보면 기존에는 FC layer를 2번거쳐 최종적으로 7 * 7 * 30 크기의 feature map을 얻는다.
    - 7 * 7 은 입력이미지를 grid 단위로 나눈것이고 각 grid 별 30차원 벡터는 5차원벡터로 표기된 box 2개와 20개의 class에대한 score값을 합친 것이다.\
    ![image](https://user-images.githubusercontent.com/70633080/103498825-619b8400-4e89-11eb-8013-0560d100390d.png)
    - 중요한 점은 5차원 박스를 예측할 때 (x,y,w,h,p) 다섯정보를 합친 벡터를 사용했다는 것이다.
    - 이는 사전에 박스는 어떤형태일 것이라는 정보없이 그냥 박스를 prediction하는 것이다. 
    - 따라서, 예측하는 박스의 크기나 위치가 중구난방이 될 수 있다. 이에 yolov2에서는 anchor box의 개념을 도입한다.
 
4. Dimension Cluster
    - anchor box는 적당히 직관적인 크기의 박스로 결정하고 비율로 설정하는 것이 일반적이다.
    - yolo v2는 여기에 learning algorithm을 적용한다.
    ![image](https://user-images.githubusercontent.com/70633080/103987721-481f7280-51d0-11eb-87a5-735bc702671b.png)
    - 이는 coco data set의 bbox에서 k-means clustering을 적용한 것이다.
    - 그 결과 anchor box를 5개로 설정하는 것이 precision과 recall측면에서 좋은 결과를 낸단 결론을 얻을 수 있었다.
    
5. Direct Location Prediction
    - 결정한 anchor box에 따라서 하나의 cell에서 5차원벡터로 이루어진 bbox를 예측한다.
    - (tx,ty,tw,th,to)를 학습을 통해 예측하며 아래와 같은 방식을 적용해 bbox를 구한다.
    ![image](https://user-images.githubusercontent.com/70633080/103987973-a9dfdc80-51d0-11eb-865c-759c1120a1d9.png)
    - 기존의 YOLO가 grid의 중심점을 예측했다면, yolo v2는 왼쪽 꼭지점으로부터 얼만큼 이동하는지를 예측한다. 
            - 이것이 bx=σ(tx) + cx가 의미하는 것이다.
    - w와 h는 사전에 정의된 box 크기를 얼만큼 비율로 조절할지를 지수승을 통해 예측한다.
            - bw=pwe^tw
    
6. Fine Grained Features
    - 기존 yolo에서는 CNN을 통과한 마지막 layer의 feature map만 사용하기 때문에 작은 물체에 대한 정보가 사라진다는 비판이 있었다.
    - yolo v2에서는 상위 layer의 feature map을 하위 feature map에 합쳐주는 **pass through layer**를 도입하였다.
    ![image](https://user-images.githubusercontent.com/70633080/103989845-a69a2000-51d3-11eb-98ba-ca5fdc2be266.png)
    - 높은 해상도를 가진 26 * 26 * 256 feature map을 13 * 13 * 2048 크기로 rescale하여 낮은 해상도의 feature map과 합쳐 13 * 13 * 3072 크기의 feature map을 만들어낸다.
    
7. Multi-Scale Traning
    - 작은 물체를 잘 detect하기 위해 yolo v2는 여러 스케일의 이미지를 학습할 수 있도록 하였다.
    - FC layer를 떼어냈기 때문에 입력이미지의 해상도에서 비교적 자유로울수 있게 되었다.
    - yolo v2는 이를 활용해 학습 시 {320, 352, ,,,,,, 608} 과 같이 32 픽셀간격으로 매 10배치시 마다 입력이미지의 해상도를 바꿔주며 학습을 진행한다.

## 결과
![image](https://user-images.githubusercontent.com/70633080/103990614-ddbd0100-51d4-11eb-8a56-b2c545b0b853.png) 
### Faster
- yolo v2가 yolo v1 보다 속도 측면에서 어떤 개선을 이루었는지 설명ㅎㄴ다.
- 기존의 pretrained된 VGG또는 Googlenet은 너무 크고 복잡하다. 따라서 새로운 CNN 아키텍처인 **Darknet**을 제시한다.
- DarkNet의 구조
![image](https://user-images.githubusercontent.com/70633080/103990881-3f7d6b00-51d5-11eb-9555-dbae2b166f03.png)
- VGG와 크게 다르지 않지만 Max Pooling을 줄이고 Conv연산을 늘렸다.
- 또한 Fully Connected layer를 제거하고 Convolution연산으로 대체하여 파라미터 수를 줄였다.

### Stronger
- yolo v2를 기반으로 총 9000개의 클래스를 분류하는 yolo9000을 어떻게 학습시켰는지 살펴본다.
1. Hierarchical Classification
![image](https://user-images.githubusercontent.com/70633080/103991226-bb77b300-51d5-11eb-9866-a665c415d840.png)
- 방대한 크기의 class에 대해 classification을 수행할 경우 계층적으로 분류작업을 수행해야한다고 제시한다.
- ImageNet 데이터를 보면 개 안에 웰시코기, 요크셔테리어 등 라벨들이 속한다.
- 이에 저자는 softmax연산을 수행할 때 전체클래스에 대해 한번에 수행하는 것이 아닌, 각 대분류 별로 수행하는 것을 제안하였다.
2. Dataset combination with word tree
- 저자는 coco와 imagenet dataset의 라벨을 트리구조를 활용해 섞는다.
![image](https://user-images.githubusercontent.com/70633080/103991778-7f911d80-51d6-11eb-9c12-e5a28a24b110.png)
3. Joint classification and detection
- 학습 부분이다. 앞서 wordtree를 이용해 9418개의 class를 가진 데이터셋을 만들어냈다. (ImageNet+COCO)
- 그러나 이중 9000개의 클래스는 ImageNet에 속했고 classification label만 붙어있는 상태이다.
- 저자는 학습과정에서 COCO Dataset이 더 많이 샘플링 되도록 하여 실제 모델이 학습하는 이미지의 비율을 4:1로 맞춰주었다. 
- classification label만 붙어있는 image의 경우 classification loss만 역전파 되게끔 하였다.
- 이를 통해 classification과 object detection task가 섞인 데이터셋을 학습할 수 있게 되었다.
4. 결과
- 19.7 mAP를 얻었다. 
- 특히 detection label이 붙은 데이터를 하나도 학습하지 못한 156개의 클래스에 대해서는 16.0 mAP라는 정확도를 달성한다.
