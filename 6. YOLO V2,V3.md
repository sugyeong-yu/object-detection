# YOLO V2
기존 YOLO모델을 보완하여 정확도를 높인 YOLO V2모델을 제시한다.\
그리고 V2모델을 기반으로 9000종류의 물체를 구분할 수 있는 YOLO9000모델을 살펴본다.\
## Better
- Batch Normalization적용
- 높은 해상도 이미지로 백본 CNN네트워크 fine tune
- Anchor Box개념 적용하여 학습 안정화
- 높은 해상도의 feature map을 낮은 해상도 feature map에 합치기

1. Batch Normalization의 적용
    - 기존 모델에서 Dropout Layer를 제거하고 Batch Normalization을 추가한다. > mAP 2% 증가

2. High Resolution Classifier
    - 기존 YOLO는 224 * 224 크기의 해상도로 학습된 VGG모델을 가져온 후, 448 * 448 크기의 이미지에 대해 OBJECT DETECTION을 수행하도록 되어있어 해상도가 맞지 않았다.
    - 이를 학습 전 Image Classification 모델을 큰 해상도이미지에 대해 fine tuning함으로써 해결하였다. > mAP 4%
    - darknet 19라는 새 모델을 사용하였다. 
    - 처음에 224 * 224 에 대한 ImageNet을 학습시킨 후, 448 * 448 에 대해 재학습을 시킨다. (이때, lr을 더 작게 설정한다.)
    - darknet19 구조 \
   ![image](https://user-images.githubusercontent.com/70633080/103993808-7e152480-51d9-11eb-9fbb-fe7dc1a4cd8d.png) 
   
3. Convolutional With Anchor Boxes
    - Faster RCNN과 비슷한 region proposal방식을 사용한다.
    - 기존 YOLO에서 FC layer를 떼고 Fully Convolutional Network형태로 prediction을 계산한다. 또, Anchor Box의 개념을 도입한다.\
    
    > +) darknet19를 학습시킬 때에는 448 * 448 이미지를 학습시키나, detection을 진행할때에는 416 * 416 으로 이미지가 바뀐다. \
    > 제일 마지막 pooling layer를 다 지운다. ( 7 * 7 grid cell은 너무 작기때문) 따라서 anchor box를 3개에서 5개로 늘리고 7 * 7 보다 큰 13 * 13 으로 feature map size를 설정하게 되면 13 * 13 * 5=845 개의 bbox를 사용할 수 있어 많은 recall을 얻을 수 있다. \
    > 또한 yolo v2는 output feature map이 짝수 * 짝수가 되는 것이 좋지 않다고 생각했다. 이는 보통 물체가 이미지의 중앙에 있는 경우가 많아 홀수 * 홀수 로 설정해주는 것이 더 좋다는 것이다.\
    > 따라서 14 * 14 가 아닌 13 * 13으로 맞춰주기 위해 448 * 448 을 416 * 416 으로 변경한 것이다.
    
    - 아래 그림을 보면 기존에는 FC layer를 2번거쳐 최종적으로 7 * 7 * 30 크기의 feature map을 얻는다.
    - 7 * 7 은 입력이미지를 grid 단위로 나눈것이고 각 grid 별 30차원 벡터는 5차원벡터로 표기된 box 2개와 20개의 class에대한 score값을 합친 것이다.\
    ![image](https://user-images.githubusercontent.com/70633080/103498825-619b8400-4e89-11eb-8013-0560d100390d.png)
    - 중요한 점은 5차원 박스를 예측할 때 (x,y,w,h,p) 다섯정보를 합친 벡터를 사용했다는 것이다.
    - 이는 사전에 박스는 어떤형태일 것이라는 정보없이 그냥 박스를 prediction하는 것이다. 
    - 따라서, 예측하는 박스의 크기나 위치가 중구난방이 될 수 있다. 이에 yolov2에서는 anchor box의 개념을 도입한다.
    
4. Dimension Cluster
    - 기존 yolo에서 anchor box는 aspect ratio와 size를 달리하여 9개의 숫자로 미리 정해주었다.
    - yolo v2는 여기에 learning algorithm을 적용한다.
    - training dataset에 있는 ground truth bounding boxes에 k-means clustering방법을 사용해 최적의 anchor box를 찾겠다는 것이다.\
    
    > ### k-means clustering
    > : 유클라디안 거리를 사용해 구하여 군집을 찾는 알고리즘\
    
    - 그러나 최적의 anchor box를 찾을 때 유클라디안 clustering으로 k-means를 할 경우 문제가 발생할 수 있다.
    ![image](https://user-images.githubusercontent.com/70633080/103995144-63dc4600-51db-11eb-8a2e-81f0bcf81429.png)
        - 파란색이 ground truth, 빨간색이 예측된 anchor box
        - 왼쪽 처럼 bounding box가 비슷한데도 불구하고 중심점끼리의 차이가 커 무시되거나 같은 group으로 보지않을 수 있다.
        - 가운데, 오른쪽 처럼 엉뚱한 anchor box가 grouping될 수 있다.
        
    - 따라서 논문에서는 IOU의 개념을 사용해 distance metric라는 방식을 제안했다. ( 유클라디안 거리가 아닌 IOU를 적용)
    ![image](https://user-images.githubusercontent.com/70633080/103995303-a7cf4b00-51db-11eb-9383-beece8add8a1.png)
    - iou를 기준으로 하면 training dataset에 있는 ground truth bbox들의 평균을 잘 계산해주어 더 좋은 anchor box를 얻을 수 있다.
    
    ![image](https://user-images.githubusercontent.com/70633080/103987721-481f7280-51d0-11eb-87a5-735bc702671b.png)
    - 이는 coco data set의 bbox에서 k-means clustering을 적용한 것이다.
    - 그 결과 anchor box를 5개로 설정하는 것이 precision과 recall측면에서 좋은 결과를 낸단 결론을 얻을 수 있었다.
    
5. Direct Location Prediction
    - Fast RCNN, Faster RCNN에서 모두 같은 bbox regression식을 이용한다.\
    ![image](https://user-images.githubusercontent.com/70633080/103995598-08f71e80-51dc-11eb-934b-0885fd476984.png)
    - 그러나 이는 d라는 함수에 제한이 없기 때문에 predicted bbox가 아래 그림과 같이 하나의 cell을 벗어나 형상될 수 있다.
    ![image](https://user-images.githubusercontent.com/70633080/103995724-32b04580-51dc-11eb-8e1b-62651255b717.png)
    - 이에 아래와 같은 방식으로 bbox regression식을 변경시킨다.
    
    - 결정한 anchor box에 따라서 하나의 cell에서 5차원벡터로 이루어진 bbox를 예측한다.
    - (tx,ty,tw,th,to)를 학습을 통해 예측하며 아래와 같은 방식을 적용해 bbox를 구한다.\
    ![image](https://user-images.githubusercontent.com/70633080/103987973-a9dfdc80-51d0-11eb-865c-759c1120a1d9.png)
    - (cx,cy) : 각 grid cell의 좌상단 끝의 offset
    - pw',ph는 prior(우선순위 앵커박스)의 w와 h이다.
    - bx',by는 ground truth에 가까워지도록 계속 학습되는 trained anchor box의 중심좌표이다.
    - 기존의 YOLO가 grid의 중심점을 예측했다면, yolo v2는 왼쪽 꼭지점으로부터 얼만큼 이동하는지를 예측한다. 
            - 이것이 bx=σ(tx) + cx가 의미하는 것이다.
    - w와 h는 사전에 정의된 box 크기를 얼만큼 비율로 조절할지를 지수승을 통해 예측한다.
            - bw=pwe^tw
    > ### sigmoid사용하는 이유\
    > sigmoid를 사용하지 않으면 predicted anchor box는 cell의 아무곳에서 생성될 수 있다.\
    > sigmoid를 사용하면 tx,ty의 범위가 0~1로 바뀌기 때문에 초기에 predicted bbox가 그려지는 범위가 해당 cell영역에 제한된다. \
    > ![image](https://user-images.githubusercontent.com/70633080/103996241-e580a380-51dc-11eb-8435-6f16913f7d8d.png)
    
6. Fine Grained Features
    - 기존 yolo에서는 CNN을 통과한 마지막 layer의 feature map만 사용하기 때문에 작은 물체에 대한 정보가 사라진다는 비판이 있었다.
    - 13 * 13 보다 큰 26 * 26 feature map에서 bbox작업을 한다. (채널수는 유지한다.)\
    ![image](https://user-images.githubusercontent.com/70633080/103996816-b7e82a00-51dd-11eb-8ed6-cc680f2bd364.png)
    - 13 * 13 일때와 채널수를 유지하여 설정해주되 26 * 26 에서는 4등분을 하여 적용한다.\
    <image src = "https://user-images.githubusercontent.com/70633080/103997483-7906a400-51de-11eb-9e75-6063f0c76887.png" width="50%" height="50%"> \
    ![image](https://user-images.githubusercontent.com/70633080/103989845-a69a2000-51d3-11eb-98ba-ca5fdc2be266.png)
    - yolo v2에서는 상위 layer의 feature map을 하위 feature map에 합쳐주는 **pass through layer**를 도입하였다.
    - 높은 해상도를 가진 26 * 26 * 256 feature map을 13 * 13 * 2048 크기로 rescale하여 낮은 해상도의 feature map과 합쳐 13 * 13 * 3072 크기의 feature map을 만들어낸다.
    
7. Multi-Scale Traning
    - 작은 물체를 잘 detect하기 위해 yolo v2는 여러 스케일의 이미지를 학습할 수 있도록 하였다.
    - FC layer를 떼어냈기 때문에 입력이미지의 해상도에서 비교적 자유로울수 있게 되었다.
    - yolo v2는 이를 활용해 학습 시 {320, 352, ,,,,,, 608} 과 같이 32 픽셀간격으로 매 10배치시 마다 입력이미지의 해상도를 바꿔주며 학습을 진행한다.

- 결과
![image](https://user-images.githubusercontent.com/70633080/103990614-ddbd0100-51d4-11eb-8a56-b2c545b0b853.png) 

## Faster
- yolo v2가 yolo v1 보다 속도 측면에서 어떤 개선을 이루었는지 설명ㅎㄴ다.
- 기존의 pretrained된 VGG또는 Googlenet은 너무 크고 복잡하다. 따라서 새로운 CNN 아키텍처인 **Darknet**을 제시한다.
- DarkNet의 구조
![image](https://user-images.githubusercontent.com/70633080/103990881-3f7d6b00-51d5-11eb-9555-dbae2b166f03.png)
- VGG와 크게 다르지 않지만 Max Pooling을 줄이고 Conv연산을 늘렸다.
- 또한 Fully Connected layer를 제거하고 Convolution연산으로 대체하여 파라미터 수를 줄였다.

## Stronger
- yolo v2를 기반으로 총 9000개의 클래스를 분류하는 yolo9000을 어떻게 학습시켰는지 살펴본다.
1. Hierarchical Classification
![image](https://user-images.githubusercontent.com/70633080/103991226-bb77b300-51d5-11eb-9866-a665c415d840.png)
- 방대한 크기의 class에 대해 classification을 수행할 경우 계층적으로 분류작업을 수행해야한다고 제시한다.
- ImageNet 데이터를 보면 개 안에 웰시코기, 요크셔테리어 등 라벨들이 속한다.
- 이에 저자는 softmax연산을 수행할 때 전체클래스에 대해 한번에 수행하는 것이 아닌, 각 대분류 별로 수행하는 것을 제안하였다.
2. Dataset combination with word tree
- 저자는 coco와 imagenet dataset의 라벨을 트리구조를 활용해 섞는다.
![image](https://user-images.githubusercontent.com/70633080/103991778-7f911d80-51d6-11eb-9c12-e5a28a24b110.png)
3. Joint classification and detection
- 학습 부분이다. 앞서 wordtree를 이용해 9418개의 class를 가진 데이터셋을 만들어냈다. (ImageNet+COCO)
- 그러나 이중 9000개의 클래스는 ImageNet에 속했고 classification label만 붙어있는 상태이다.
- 저자는 학습과정에서 COCO Dataset이 더 많이 샘플링 되도록 하여 실제 모델이 학습하는 이미지의 비율을 4:1로 맞춰주었다. 
- classification label만 붙어있는 image의 경우 classification loss만 역전파 되게끔 하였다.
- 이를 통해 classification과 object detection task가 섞인 데이터셋을 학습할 수 있게 되었다.
4. 결과
- 19.7 mAP를 얻었다. 
- 특히 detection label이 붙은 데이터를 하나도 학습하지 못한 156개의 클래스에 대해서는 16.0 mAP라는 정확도를 달성한다.
