# YOLO V2
기존 YOLO모델을 보완하여 정확도를 높인 YOLO V2모델을 제시한다.\
그리고 V2모델을 기반으로 9000종류의 물체를 구분할 수 있는 YOLO9000모델을 살펴본다.\
## 발전
- Batch Normalization적용
- 높은 해상도 이미지로 백본 CNN네트워크 fine tune
- Anchor Box개념 적용하여 학습 안정화
- 높은 해상도의 feature map을 낮은 해상도 feature map에 합치기

1. Batch Normalization의 적용
    - 기존 모델에서 Dropout Layer를 제거하고 Batch Normalization을 추가한다. > mAP 2% 증가

2. High Resolution Classifier
    - 기존 YOLO는 224 * 224 크기의 해상도로 학습된 VGG모델을 가져온 후, 448 * 448 크기의 이미지에 대해 OBJECT DETECTION을 수행하도록 되어있어 해상도가 맞지 않았다.\
    - 이를 학습 전 Image Classification 모델을 큰 해상도이미지에 대해 fine tuning함으로써 해결하였다. > mAP 4%
   
3. Convolutional With Anchor Boxes
    - 기존 YOLO에서 FC layer를 떼고 Fully Convolutional Network형태로 prediction을 계산한다. 또, Anchor Box의 개념을 도입한다.
    - 아래 그림을 보면 기존에는 FC layer를 2번거쳐 최종적으로 7 * 7 * 30 크기의 feature map을 얻는다.
    - 7 * 7 은 입력이미지를 grid 단위로 나눈것이고 각 grid 별 30차원 벡터는 5차원벡터로 표기된 box 2개와 20개의 class에대한 score값을 합친 것이다.\
    ![image](https://user-images.githubusercontent.com/70633080/103498825-619b8400-4e89-11eb-8013-0560d100390d.png)
    - 중요한 점은 5차원 박스를 예측할 때 (x,y,w,h,p) 다섯정보를 합친 벡터를 사용했다는 것이다.
    - 이는 사전에 박스는 어떤형태일 것이라는 정보없이 그냥 박스를 prediction하는 것이다. 
    - 따라서, 예측하는 박스의 크기나 위치가 중구난방이 될 수 있다. 이에 yolov2에서는 anchor box의 개념을 도입한다.
 
4. Dimension Cluster
    - anchor box는 적당히 직관적인 크기의 박스로 결정하고 비율로 설정하는 것이 일반적이다.
    - yolo v2는 여기에 learning algorithm을 적용한다.
    ![image](https://user-images.githubusercontent.com/70633080/103987721-481f7280-51d0-11eb-87a5-735bc702671b.png)
    - 이는 coco data set의 bbox에서 k-means clustering을 적용한 것이다.
    - 그 결과 anchor box를 5개로 설정하는 것이 precision과 recall측면에서 좋은 결과를 낸단 결론을 얻을 수 있었다.
    
5. Direct Location Prediction
    - 결정한 anchor box에 따라서 하나의 cell에서 5차원벡터로 이루어진 bbox를 예측한다.
    - (tx,ty,tw,th,to)를 학습을 통해 예측하며 아래와 같은 방식을 적용해 bbox를 구한다.
    ![image](https://user-images.githubusercontent.com/70633080/103987973-a9dfdc80-51d0-11eb-865c-759c1120a1d9.png)
    - 기존의 YOLO가 grid의 중심점을 예측했다면, yolo v2는 왼쪽 꼭지점으로부터 얼만큼 이동하는지를 예측한다. 
            - 이것이 bx=σ(tx) + cx가 의미하는 것이다.
    - w와 h는 사전에 정의된 box 크기를 얼만큼 비율로 조절할지를 지수승을 통해 예측한다.
            - bw=pwe^tw
    
6. Fine Grained Features
    - 기존 yolo에서는 CNN을 통과한 마지막 layer의 feature map만 사용하기 때문에 작은 물체에 대한 정보가 사라진다는 비판이 있었다.
    - yolo v2에서는 상위 layer의 feature map을 하위 feature map에 합쳐주는 **pass through layer**를 도입하였다.
    ![image](https://user-images.githubusercontent.com/70633080/103989845-a69a2000-51d3-11eb-98ba-ca5fdc2be266.png)
    - 높은 해상도를 가진 26 * 26 * 256 feature map을 13 * 13 * 2048 크기로 rescale하여 낮은 해상도의 feature map과 합쳐 13 * 13 * 3072 크기의 feature map을 만들어낸다.
